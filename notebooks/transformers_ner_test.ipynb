{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some config variables\n",
    "MAX_LEN = 75\n",
    "bs      = 32\n",
    "tag2idx = {'B-art': 0,\n",
    " 'B-eve': 1,\n",
    " 'B-geo': 2,\n",
    " 'B-gpe': 3,\n",
    " 'B-nat': 4,\n",
    " 'B-org': 5,\n",
    " 'B-per': 6,\n",
    " 'B-tim': 7,\n",
    " 'I-art': 8,\n",
    " 'I-eve': 9,\n",
    " 'I-geo': 10,\n",
    " 'I-gpe': 11,\n",
    " 'I-nat': 12,\n",
    " 'I-org': 13,\n",
    " 'I-per': 14,\n",
    " 'I-tim': 15,\n",
    " 'O': 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2tag = {}\n",
    "for key in list(tag2idx.keys()) :\n",
    "    idx2tag[tag2idx[key]] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model     = BertForTokenClassification.from_pretrained('bert-base-uncased',num_labels=len(tag2idx))\n",
    "model.load_state_dict(torch.load(\"ner.dataset.4.pth\",map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "print(\"Ready to use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = \"He said last week's tsunami and the massive underwater earthquake that triggered it has affected millions in Asia and Africa.\"\n",
    "# sentence = \"In Beirut, a string of officials voiced their anger, while at the United Nations summit in New York, Prime Minister Fouad Siniora said the Lebanese people are resolute in preventing such attempts from destroying their spirit.\"\n",
    "# sentence = 'Lebanon has suffered a series of bombings since the massive explosion in February that killed former Prime Minister Rafik Hariri and 20 other people.'\n",
    "# sentence = \"The attacks came as Britain 's Foreign Secretary Jack Straw met in Baghdad with President Jalal Talabani about the slow progress in forming a new Iraqi government .\"\n",
    "# sentence = \"In another development, an Italian court has scheduled an August 17 extradition hearing for one of the suspects in the July 21 failed London bombings.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"sentence\": \"He said last week's tsunami and the massive underwater earthquake that triggered it has affected millions in Asia and Africa.\",\n",
      " \"predictions\": [\n",
      "  {\n",
      "   \"token\": \"he\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"said\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"last\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"week\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"'\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"s\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"tsunami\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"and\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"the\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"massive\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"underwater\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"earthquake\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"that\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"triggered\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"it\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"has\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"affected\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"millions\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"in\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"asia\",\n",
      "   \"label\": \"B-geo\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"and\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"africa\",\n",
      "   \"label\": \"B-geo\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \".\",\n",
      "   \"label\": \"O\"\n",
      "  }\n",
      " ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# The tokenizer encoding is leading to screwy results when the labels are propagated out\n",
    "# tok_sen   = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "# tok_senl  = [tokenizer.decode([tok_sen[i]]) for i in range(len(tok_sen))]\n",
    "\n",
    "tokenized_text = tokenizer.encode(sentence, add_special_tokens=False) # [s.lower() for s in sentence.split(\" \")]\n",
    "tok_senl       = [tokenizer.decode([tokenized_text[i]]) for i in range(len(tokenized_text))]\n",
    "input_ids      = torch.tensor([tokenized_text])\n",
    "\n",
    "# input_ids = torch.tensor(tok_sen).unsqueeze(0)  # Batch size 1\n",
    "\n",
    "labels    = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1\n",
    "outputs   = model(input_ids, labels=labels)\n",
    "\n",
    "loss, scores = outputs[:2]\n",
    "\n",
    "# Have to filter out the padding\n",
    "pred_labels = np.array([np.argmax(scores[0].detach().numpy()[j]) for j in range(len(scores[0]))])\n",
    "\n",
    "# Sanity check\n",
    "assert(len(pred_labels)==len(tokenized_text))\n",
    "\n",
    "# Convert the labels back to text representation\n",
    "txt_labels = [idx2tag[i] for i in pred_labels]\n",
    "\n",
    "# Create an output JSON for the tokenized text\n",
    "odict = {'sentence':sentence,'predictions':[]}\n",
    "for tt in zip(tok_senl,txt_labels) :\n",
    "    odict['predictions'].append({\n",
    "        'token' : tt[0],\n",
    "        'label' : tt[1]\n",
    "    })\n",
    "print(json.dumps(odict,indent=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
