{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main module to train and test NER based on Bert embeddings\n",
    "#### By Isar Nejadgholi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from model import Net\n",
    "from data_load import NerDataset, pad\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if gpu is available, if not set on cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print('GPU Type:     ',torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "    print('Number of gpus:   ', torch.cuda.device_count())\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    !export CUDA_VISIBLE_DEVICES=1\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set bert model and dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_name ='i2b2'  #i2b2 or 'medmention'\n",
    "Bert_Model_name ='clinical_15'  #'general' or 'biobert' or 'clinical_10' or 'clinical_15' or 'NCBI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train, test, dev sets and model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETTINGS \n",
    "\n",
    "if Dataset_name =='i2b2':\n",
    "    trainset = \"data/i2b2_train_total.txt\"\n",
    "    validset = \"data/i2b2_train_total.txt\"\n",
    "    testset = \"data/i2b2_test_total.txt\"\n",
    "else:\n",
    "    trainset = \"data/medmention_PA_train.txt\"\n",
    "    validset = \"data/medmention_PA_valiadtion.txt\"\n",
    "    testset = \"data/medmention_PA_test.txt\"\n",
    "    \n",
    "    \n",
    "saves_file = 'saved_models/general_uncased_i2b2.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion,tokenizer):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(iterator):\n",
    "        words, x, is_heads, tags, y, seqlens = batch\n",
    "        _y = y # for monitoring\n",
    "        optimizer.zero_grad()\n",
    "        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n",
    "\n",
    "        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n",
    "        y = y.view(-1)  # (N*T,)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i==0:\n",
    "            print(\"=====sanity check======\")\n",
    "            print(\"words:\", words[0])\n",
    "            print(\"x:\", x.cpu().numpy()[0][:seqlens[0]])\n",
    "            print(\"tokens:\", tokenizer.convert_ids_to_tokens(x.cpu().numpy()[0])[:seqlens[0]])\n",
    "            print(\"is_heads:\", is_heads[0])\n",
    "            print(\"y:\", _y.cpu().numpy()[0][:seqlens[0]])\n",
    "            print(\"tags:\", tags[0])\n",
    "            print(\"seqlen:\", seqlens[0])\n",
    "            print(\"=======================\")\n",
    "\n",
    "\n",
    "        if i%10==0: # monitoring\n",
    "            print(f\"step: {i}, loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, iterator, f):\n",
    "    model.eval()\n",
    "\n",
    "    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            words, x, is_heads, tags, y, seqlens = batch\n",
    "\n",
    "            _, _, y_hat = model(x, y)  # y_hat: (N, T)\n",
    "\n",
    "            Words.extend(words)\n",
    "            Is_heads.extend(is_heads)\n",
    "            Tags.extend(tags)\n",
    "            Y.extend(y.numpy().tolist())\n",
    "            Y_hat.extend(y_hat.cpu().numpy().tolist())\n",
    "\n",
    "    ## gets results and save\n",
    "    true_tag = []\n",
    "    pred_tag = []\n",
    "    with open(\"temp\", 'w') as fout:\n",
    "        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n",
    "            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n",
    "            preds = [train_dataset.idx2tag[hat] for hat in y_hat]\n",
    "            assert len(preds)==len(words.split())==len(tags.split())\n",
    "            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n",
    "                fout.write(f\"{w} {t} {p}\\n\")\n",
    "            fout.write(\"\\n\")\n",
    "            true_tag.append(tags.split()[1:-1])\n",
    "            pred_tag.append(preds[1:-1])\n",
    "\n",
    "    ## calc metric\n",
    "    y_true =  np.array([train_dataset.tag2idx[line.split()[1]] for line in open(\"temp\", 'r').read().splitlines() if len(line) > 0])\n",
    "    y_pred =  np.array([train_dataset.tag2idx[line.split()[2]] for line in open(\"temp\", 'r').read().splitlines() if len(line) > 0])\n",
    "\n",
    "    num_proposed = len(y_pred[y_pred>1])\n",
    "    num_correct = (np.logical_and(y_true==y_pred, y_true>1)).astype(np.int).sum()\n",
    "    num_gold = len(y_true[y_true>1])\n",
    "\n",
    "    print(f\"num_proposed:{num_proposed}\")\n",
    "    print(f\"num_correct:{num_correct}\")\n",
    "    print(f\"num_gold:{num_gold}\")\n",
    "    try:\n",
    "        precision = num_correct / num_proposed\n",
    "    except ZeroDivisionError:\n",
    "        precision = 1.0\n",
    "\n",
    "    try:\n",
    "        recall = num_correct / num_gold\n",
    "    except ZeroDivisionError:\n",
    "        recall = 1.0\n",
    "\n",
    "    try:\n",
    "        f1 = 2*precision*recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        if precision*recall==0:\n",
    "            f1=1.0\n",
    "        else:\n",
    "            f1=0\n",
    "\n",
    "    final = f + \".P%.2f_R%.2f_F%.2f\" %(precision, recall, f1)\n",
    "    with open(final, 'w') as fout:\n",
    "        result = open(\"temp\", \"r\").read()\n",
    "        fout.write(f\"{result}\\n\")\n",
    "\n",
    "        fout.write(f\"precision={precision}\\n\")\n",
    "        fout.write(f\"recall={recall}\\n\")\n",
    "        fout.write(f\"f1={f1}\\n\")\n",
    "\n",
    "    os.remove(\"temp\")\n",
    "\n",
    "    print(\"precision=%.2f\"%precision)\n",
    "    print(\"recall=%.2f\"%recall)\n",
    "    print(\"f1=%.2f\"%f1)\n",
    "    return precision, recall, f1,true_tag, pred_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initiate parameters, model and data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_size =32\n",
    "lr =0.00005\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "\n",
    "finetuning = True\n",
    "logdir = \"checkpoints/01\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Net(bert_name = Bert_Model_name,  device=device, finetuning=finetuning,data_name = Dataset_name)\n",
    "\n",
    "# send the model to DataParallel only if you are training on multiple gpus\n",
    "model = nn.DataParallel(model)  \n",
    "\n",
    "train_dataset = NerDataset(trainset,bert_name = Bert_Model_name,data_name = Dataset_name)\n",
    "eval_dataset = NerDataset(validset,bert_name = Bert_Model_name,data_name = Dataset_name)\n",
    "\n",
    "train_iter = data.DataLoader(dataset=train_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=True,\n",
    "                             num_workers=0,\n",
    "                             collate_fn=pad)\n",
    "eval_iter = data.DataLoader(dataset=eval_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             num_workers=0,\n",
    "                             shuffle=False,\n",
    "                             collate_fn=pad)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'<PAD>': 0,\n",
       "  'O': 1,\n",
       "  'B-problem': 2,\n",
       "  'I-problem': 3,\n",
       "  'B-treatment': 4,\n",
       "  'I-treatment': 5,\n",
       "  'B-test': 6,\n",
       "  'I-test': 7},\n",
       " 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.tag2idx, train_dataset.label_num \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/lm_finetuning/simple_lm_finetuning.py\n",
    "if finetuning:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "    n_epochs =4\n",
    "    \n",
    "    num_train_optimization_steps = int(\n",
    "len(train_dataset) / batch_size / gradient_accumulation_steps) * n_epochs\n",
    "\n",
    "    optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                 lr=0.00005,\n",
    "                 warmup=0.1,\n",
    "                 t_total=num_train_optimization_steps)\n",
    "     \n",
    "else:\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.00005) \n",
    "    n_epochs =4\n",
    "    \n",
    "    \n",
    "    \n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.tag2idx['<PAD>'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====sanity check======\n",
      "words: [CLS] Ronchorous breath sounds left chest . [SEP]\n",
      "x: [  101  6413 22811  2285  2184  3807  1286  2229   119   102]\n",
      "tokens: ['[CLS]', 'Ron', '##chor', '##ous', 'breath', 'sounds', 'left', 'chest', '.', '[SEP]']\n",
      "is_heads: [1, 1, 0, 0, 1, 1, 1, 1, 1, 1]\n",
      "y: [0 2 0 0 3 3 3 3 1 0]\n",
      "tags: <PAD> B-problem I-problem I-problem I-problem I-problem O <PAD>\n",
      "seqlen: 10\n",
      "=======================\n",
      "step: 0, loss: 2.456310987472534\n",
      "step: 10, loss: 1.0843297243118286\n",
      "step: 20, loss: 0.8022581338882446\n",
      "step: 30, loss: 0.7786921262741089\n",
      "step: 40, loss: 1.188483476638794\n",
      "step: 50, loss: 0.8169649839401245\n",
      "step: 60, loss: 0.4862866699695587\n",
      "step: 70, loss: 0.49962860345840454\n",
      "step: 80, loss: 0.32892483472824097\n",
      "step: 90, loss: 0.31139689683914185\n",
      "step: 100, loss: 0.319164514541626\n",
      "step: 110, loss: 0.2556055784225464\n",
      "step: 120, loss: 0.16158416867256165\n",
      "step: 130, loss: 0.2105257362127304\n",
      "step: 140, loss: 0.2423766553401947\n",
      "step: 150, loss: 0.18080030381679535\n",
      "step: 160, loss: 0.21201051771640778\n",
      "step: 170, loss: 0.23220489919185638\n",
      "step: 180, loss: 0.2737168073654175\n",
      "step: 190, loss: 0.17296937108039856\n",
      "step: 200, loss: 0.1398722529411316\n",
      "step: 210, loss: 0.10638922452926636\n",
      "step: 220, loss: 0.11745266616344452\n",
      "step: 230, loss: 0.20970414578914642\n",
      "step: 240, loss: 0.15099729597568512\n",
      "step: 250, loss: 0.32344764471054077\n",
      "step: 260, loss: 0.17490409314632416\n",
      "step: 270, loss: 0.3461909592151642\n",
      "step: 280, loss: 0.08811366558074951\n",
      "step: 290, loss: 0.16205504536628723\n",
      "step: 300, loss: 0.1336406022310257\n",
      "step: 310, loss: 0.25092434883117676\n",
      "step: 320, loss: 0.2584168314933777\n",
      "step: 330, loss: 0.18346884846687317\n",
      "step: 340, loss: 0.20611725747585297\n",
      "step: 350, loss: 0.14597845077514648\n",
      "step: 360, loss: 0.11624214798212051\n",
      "step: 370, loss: 0.1179019957780838\n",
      "step: 380, loss: 0.08590926229953766\n",
      "step: 390, loss: 0.27767637372016907\n",
      "step: 400, loss: 0.15854358673095703\n",
      "step: 410, loss: 0.15492914617061615\n",
      "step: 420, loss: 0.10226337611675262\n",
      "step: 430, loss: 0.15856778621673584\n",
      "step: 440, loss: 0.11777522414922714\n",
      "step: 450, loss: 0.1830715537071228\n",
      "step: 460, loss: 0.16496671736240387\n",
      "step: 470, loss: 0.05076344683766365\n",
      "step: 480, loss: 0.15236562490463257\n",
      "step: 490, loss: 0.15781927108764648\n",
      "step: 500, loss: 0.16137352585792542\n",
      "step: 510, loss: 0.208733931183815\n",
      "=========eval at epoch=1=========\n",
      "num_proposed:34723\n",
      "num_correct:32623\n",
      "num_gold:34758\n",
      "precision=0.94\n",
      "recall=0.94\n",
      "f1=0.94\n",
      "=====sanity check======\n",
      "words: [CLS] O [SEP]\n",
      "x: [101 152 102]\n",
      "tokens: ['[CLS]', 'O', '[SEP]']\n",
      "is_heads: [1, 1, 1]\n",
      "y: [0 1 0]\n",
      "tags: <PAD> O <PAD>\n",
      "seqlen: 3\n",
      "=======================\n",
      "step: 0, loss: 0.13005748391151428\n",
      "step: 10, loss: 0.05728711187839508\n",
      "step: 20, loss: 0.13585563004016876\n",
      "step: 30, loss: 0.046291328966617584\n",
      "step: 40, loss: 0.04264809191226959\n",
      "step: 50, loss: 0.04213632643222809\n",
      "step: 60, loss: 0.05471479147672653\n",
      "step: 70, loss: 0.039811503142118454\n",
      "step: 80, loss: 0.13131965696811676\n",
      "step: 90, loss: 0.09111971408128738\n",
      "step: 100, loss: 0.09706071019172668\n",
      "step: 110, loss: 0.07697471976280212\n",
      "step: 120, loss: 0.0950411930680275\n",
      "step: 130, loss: 0.13442572951316833\n",
      "step: 140, loss: 0.03325340524315834\n",
      "step: 150, loss: 0.0825372040271759\n",
      "step: 160, loss: 0.08462196588516235\n",
      "step: 170, loss: 0.061170339584350586\n",
      "step: 180, loss: 0.08749496936798096\n",
      "step: 190, loss: 0.06270641088485718\n",
      "step: 200, loss: 0.0622435137629509\n",
      "step: 210, loss: 0.04998113587498665\n",
      "step: 220, loss: 0.0887036994099617\n",
      "step: 230, loss: 0.059119876474142075\n",
      "step: 240, loss: 0.18215781450271606\n",
      "step: 250, loss: 0.055619288235902786\n",
      "step: 260, loss: 0.03247857466340065\n",
      "step: 270, loss: 0.06904451549053192\n",
      "step: 280, loss: 0.08166830241680145\n",
      "step: 290, loss: 0.08246410638093948\n",
      "step: 300, loss: 0.07880544662475586\n",
      "step: 310, loss: 0.04679611325263977\n",
      "step: 320, loss: 0.14947988092899323\n",
      "step: 330, loss: 0.019468041136860847\n",
      "step: 340, loss: 0.015892604365944862\n",
      "step: 350, loss: 0.2738991975784302\n",
      "step: 360, loss: 0.06146121397614479\n",
      "step: 370, loss: 0.03221551328897476\n",
      "step: 380, loss: 0.09536266326904297\n",
      "step: 390, loss: 0.018088096752762794\n",
      "step: 400, loss: 0.15889377892017365\n",
      "step: 410, loss: 0.04078013449907303\n",
      "step: 420, loss: 0.06960429251194\n",
      "step: 430, loss: 0.16039220988750458\n",
      "step: 440, loss: 0.11110886931419373\n",
      "step: 450, loss: 0.06434738636016846\n",
      "step: 460, loss: 0.1434808224439621\n",
      "step: 470, loss: 0.09941775351762772\n",
      "step: 480, loss: 0.06846951693296432\n",
      "step: 490, loss: 0.045004647225141525\n",
      "step: 500, loss: 0.09141121804714203\n",
      "step: 510, loss: 0.07238233089447021\n",
      "=========eval at epoch=2=========\n",
      "num_proposed:35228\n",
      "num_correct:34096\n",
      "num_gold:34758\n",
      "precision=0.97\n",
      "recall=0.98\n",
      "f1=0.97\n",
      "=====sanity check======\n",
      "words: [CLS] 5. Prophylaxis : [SEP]\n",
      "x: [  101   126   119  5096 22192 22731  1548   131   102]\n",
      "tokens: ['[CLS]', '5', '.', 'Pro', '##phy', '##lax', '##is', ':', '[SEP]']\n",
      "is_heads: [1, 1, 0, 1, 0, 0, 0, 1, 1]\n",
      "y: [0 1 0 1 0 0 0 1 0]\n",
      "tags: <PAD> O O O <PAD>\n",
      "seqlen: 9\n",
      "=======================\n",
      "step: 0, loss: 0.023262139409780502\n",
      "step: 10, loss: 0.026297958567738533\n",
      "step: 20, loss: 0.036330971866846085\n",
      "step: 30, loss: 0.005827039480209351\n",
      "step: 40, loss: 0.008979659527540207\n",
      "step: 50, loss: 0.018898051232099533\n",
      "step: 60, loss: 0.021228332072496414\n",
      "step: 70, loss: 0.026776034384965897\n",
      "step: 80, loss: 0.04051673412322998\n",
      "step: 90, loss: 0.08601795136928558\n",
      "step: 100, loss: 0.03839859366416931\n",
      "step: 110, loss: 0.027303267270326614\n",
      "step: 120, loss: 0.04908463731408119\n",
      "step: 130, loss: 0.08373674750328064\n",
      "step: 140, loss: 0.02368815429508686\n",
      "step: 150, loss: 0.024747980758547783\n",
      "step: 160, loss: 0.02025524154305458\n",
      "step: 170, loss: 0.03402712941169739\n",
      "step: 180, loss: 0.0031871481332927942\n",
      "step: 190, loss: 0.006460204720497131\n",
      "step: 200, loss: 0.00764186168089509\n",
      "step: 210, loss: 0.06454741954803467\n",
      "step: 220, loss: 0.022294392809271812\n",
      "step: 230, loss: 0.0303899385035038\n",
      "step: 240, loss: 0.005539898294955492\n",
      "step: 250, loss: 0.004780677612870932\n",
      "step: 260, loss: 0.008691608905792236\n",
      "step: 270, loss: 0.013114537112414837\n",
      "step: 280, loss: 0.04603791609406471\n",
      "step: 290, loss: 0.06089816242456436\n",
      "step: 300, loss: 0.02659483440220356\n",
      "step: 310, loss: 0.04118617996573448\n",
      "step: 320, loss: 0.007456200663000345\n",
      "step: 330, loss: 0.005260137375444174\n",
      "step: 340, loss: 0.036695800721645355\n",
      "step: 350, loss: 0.005256799049675465\n",
      "step: 360, loss: 0.05959382653236389\n",
      "step: 370, loss: 0.0249455738812685\n",
      "step: 380, loss: 0.06763295084238052\n",
      "step: 390, loss: 0.027207333594560623\n",
      "step: 400, loss: 0.057835910469293594\n",
      "step: 410, loss: 0.022179320454597473\n",
      "step: 420, loss: 0.10309269279241562\n",
      "step: 430, loss: 0.03983697667717934\n",
      "step: 440, loss: 0.05034622922539711\n",
      "step: 450, loss: 0.07069608569145203\n",
      "step: 460, loss: 0.01389055885374546\n",
      "step: 470, loss: 0.028665602207183838\n",
      "step: 480, loss: 0.008039574138820171\n",
      "step: 490, loss: 0.07203999906778336\n",
      "step: 500, loss: 0.046534135937690735\n",
      "step: 510, loss: 0.011296259239315987\n",
      "=========eval at epoch=3=========\n",
      "num_proposed:35010\n",
      "num_correct:34465\n",
      "num_gold:34758\n",
      "precision=0.98\n",
      "recall=0.99\n",
      "f1=0.99\n",
      "=====sanity check======\n",
      "words: [CLS] There was no diplopia , visual loss , speech abnormality or sensory change in her history . [SEP]\n",
      "x: [  101  1247  1108  1185 20866 13200  1465   117  5173  2445   117  4055\n",
      " 22832  1785  1137 19144  1849  1107  1123  1607   119   102]\n",
      "tokens: ['[CLS]', 'There', 'was', 'no', 'dip', '##lop', '##ia', ',', 'visual', 'loss', ',', 'speech', 'abnormal', '##ity', 'or', 'sensory', 'change', 'in', 'her', 'history', '.', '[SEP]']\n",
      "is_heads: [1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "y: [0 1 1 1 2 0 0 1 2 3 1 2 3 0 1 2 3 1 1 1 1 0]\n",
      "tags: <PAD> O O O B-problem O B-problem I-problem O B-problem I-problem O B-problem I-problem O O O O <PAD>\n",
      "seqlen: 22\n",
      "=======================\n",
      "step: 0, loss: 0.011451806873083115\n",
      "step: 10, loss: 0.0040908497758209705\n",
      "step: 20, loss: 0.0064928787760436535\n",
      "step: 30, loss: 0.03961993753910065\n",
      "step: 40, loss: 0.03340185433626175\n",
      "step: 50, loss: 0.016443731263279915\n",
      "step: 60, loss: 0.027487335726618767\n",
      "step: 70, loss: 0.01333966851234436\n",
      "step: 80, loss: 0.03857631981372833\n",
      "step: 90, loss: 0.0017880984814837575\n",
      "step: 100, loss: 0.04840914532542229\n",
      "step: 110, loss: 0.0040624444372951984\n",
      "step: 120, loss: 0.009920480661094189\n",
      "step: 130, loss: 0.019611891359090805\n",
      "step: 140, loss: 0.07942965626716614\n",
      "step: 150, loss: 0.014023714698851109\n",
      "step: 160, loss: 0.02592146210372448\n",
      "step: 170, loss: 0.008617919869720936\n",
      "step: 180, loss: 0.04144895449280739\n",
      "step: 190, loss: 0.0016434431308880448\n",
      "step: 200, loss: 0.0025513265281915665\n",
      "step: 210, loss: 0.0070906709879636765\n",
      "step: 220, loss: 0.027253154665231705\n",
      "step: 230, loss: 0.00636823708191514\n",
      "step: 240, loss: 0.03417200595140457\n",
      "step: 250, loss: 0.004364434164017439\n",
      "step: 260, loss: 0.014327175915241241\n",
      "step: 270, loss: 0.016054442152380943\n",
      "step: 280, loss: 0.004540880676358938\n",
      "step: 290, loss: 0.02235911227762699\n",
      "step: 300, loss: 0.005626561585813761\n",
      "step: 310, loss: 0.018148742616176605\n",
      "step: 320, loss: 0.008868980221450329\n",
      "step: 330, loss: 0.00876760482788086\n",
      "step: 340, loss: 0.04093620181083679\n",
      "step: 350, loss: 0.0370902344584465\n",
      "step: 360, loss: 0.013114308007061481\n",
      "step: 370, loss: 0.0479581356048584\n",
      "step: 380, loss: 0.07129479199647903\n",
      "step: 390, loss: 0.02695472538471222\n",
      "step: 400, loss: 0.013050546869635582\n",
      "step: 410, loss: 0.02982533909380436\n",
      "step: 420, loss: 0.015549816191196442\n",
      "step: 430, loss: 0.02056664600968361\n",
      "step: 440, loss: 0.009214607067406178\n",
      "step: 450, loss: 0.033825505524873734\n",
      "step: 460, loss: 0.0874302089214325\n",
      "step: 470, loss: 0.013389216735959053\n",
      "step: 480, loss: 0.004481519106775522\n",
      "step: 490, loss: 0.004035239573568106\n",
      "step: 500, loss: 0.04970983415842056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 510, loss: 0.01735931821167469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.\n",
      "Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========eval at epoch=4=========\n",
      "num_proposed:34767\n",
      "num_correct:34491\n",
      "num_gold:34758\n",
      "precision=0.99\n",
      "recall=0.99\n",
      "f1=0.99\n",
      "weights were saved to checkpoints/01/4.pt\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs+1):\n",
    "    \n",
    "    train(model, train_iter, optimizer, criterion,train_dataset.tokenizer)\n",
    "\n",
    "    print(f\"=========eval at epoch={epoch}=========\")\n",
    "    if not os.path.exists(logdir): os.makedirs(logdir)\n",
    "    fname = os.path.join(logdir, str(epoch))\n",
    "    precision, recall, f1, true, pred= eval(model, eval_iter, fname)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{fname}.pt\")\n",
    "print(f\"weights were saved to {fname}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional Info when using cuda\n",
    "\n",
    "#print(torch.cuda.get_device_name(0))\n",
    "#print('Memory Usage:')\n",
    "#print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "#print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_proposed:64606\n",
      "num_correct:58436\n",
      "num_gold:64811\n",
      "precision=0.90\n",
      "recall=0.90\n",
      "f1=0.90\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_dataset = NerDataset(testset,bert_name = Bert_Model_name,data_name = Dataset_name)\n",
    "test_iter = data.DataLoader(dataset=test_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             num_workers=0,\n",
    "                             shuffle=False,\n",
    "                             collate_fn=pad)\n",
    "\n",
    "precision, recall, f1,true_tags,pred_tags = eval(model, test_iter, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_list_true_tags = [item for sublist in true_tags for item in sublist]\n",
    "flatten_list_pred_tags = [item for sublist in pred_tags for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   B-problem       0.92      0.92      0.92     12592\n",
      "      B-test       0.91      0.91      0.91      9225\n",
      " B-treatment       0.91      0.90      0.91      9344\n",
      "   I-problem       0.90      0.92      0.91     17684\n",
      "      I-test       0.90      0.88      0.89      8012\n",
      " I-treatment       0.88      0.85      0.87      7954\n",
      "           O       0.98      0.98      0.98    203026\n",
      "\n",
      "    accuracy                           0.96    267837\n",
      "   macro avg       0.91      0.91      0.91    267837\n",
      "weighted avg       0.96      0.96      0.96    267837\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(flatten_list_true_tags,flatten_list_pred_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "treatment       0.87      0.88      0.87      9344\n",
      "  problem       0.86      0.88      0.87     12592\n",
      "     test       0.87      0.88      0.87      9225\n",
      "\n",
      "micro avg       0.86      0.88      0.87     31161\n",
      "macro avg       0.86      0.88      0.87     31161\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "print(classification_report(true_tags,pred_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), saves_file )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
