{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1048565</th>\n",
       "      <td>Sentence: 47958</td>\n",
       "      <td>impact</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048566</th>\n",
       "      <td>Sentence: 47958</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048567</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>Indian</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048568</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>forces</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048569</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>said</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>they</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>responded</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>attack</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sentence #       Word  POS    Tag\n",
       "1048565  Sentence: 47958     impact   NN      O\n",
       "1048566  Sentence: 47958          .    .      O\n",
       "1048567  Sentence: 47959     Indian   JJ  B-gpe\n",
       "1048568  Sentence: 47959     forces  NNS      O\n",
       "1048569  Sentence: 47959       said  VBD      O\n",
       "1048570  Sentence: 47959       they  PRP      O\n",
       "1048571  Sentence: 47959  responded  VBD      O\n",
       "1048572  Sentence: 47959         to   TO      O\n",
       "1048573  Sentence: 47959        the   DT      O\n",
       "1048574  Sentence: 47959     attack   NN      O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from tqdm import tqdm, trange\n",
    "\n",
    "data = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\").fillna(method=\"ffill\")\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_vals = sorted(list(set(data[\"Tag\"].values)))\n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-art': 0,\n",
       " 'B-eve': 1,\n",
       " 'B-geo': 2,\n",
       " 'B-gpe': 3,\n",
       " 'B-nat': 4,\n",
       " 'B-org': 5,\n",
       " 'B-per': 6,\n",
       " 'B-tim': 7,\n",
       " 'I-art': 8,\n",
       " 'I-eve': 9,\n",
       " 'I-geo': 10,\n",
       " 'I-gpe': 11,\n",
       " 'I-nat': 12,\n",
       " 'I-org': 13,\n",
       " 'I-per': 14,\n",
       " 'I-tim': 15,\n",
       " 'O': 16}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2tag = {}\n",
    "for key in list(tag2idx.keys()) :\n",
    "    idx2tag[tag2idx[key]] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-art',\n",
       " 1: 'B-eve',\n",
       " 2: 'B-geo',\n",
       " 3: 'B-gpe',\n",
       " 4: 'B-nat',\n",
       " 5: 'B-org',\n",
       " 6: 'B-per',\n",
       " 7: 'B-tim',\n",
       " 8: 'I-art',\n",
       " 9: 'I-eve',\n",
       " 10: 'I-geo',\n",
       " 11: 'I-gpe',\n",
       " 12: 'I-nat',\n",
       " 13: 'I-org',\n",
       " 14: 'I-per',\n",
       " 15: 'I-tim',\n",
       " 16: 'O'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 75\n",
    "bs      = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>Need to change the input ID / tag creation to reflect the fact that BERT's tokenizer $\\neq$ splitting by spaces. Some words will be tokenized into multiple segments which need to be assigned either O's or I- tags as necessary. </font>**\n",
    "\n",
    "**Then we can pad sequences once it's guaranteed the IDs and labels have equal length.**\n",
    "\n",
    "**NB: This means I will have to retain the model on Colab using this new splitting algorithm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d6d6e6beb7b4e788aa3dd753e448c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=47959), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = []\n",
    "labels          = []\n",
    "for sentence in tqdm(getter.sentences) :\n",
    "    # Split into tokens by spaces\n",
    "    # Now split each token into sub-tokens using the tokenizer\n",
    "    # such that any new sub-tokens receive either a \"O\" or \"I-\"\n",
    "    # label as necessary\n",
    "    words  = []\n",
    "    lls    = []\n",
    "    for i in range(len(sentence)) :\n",
    "        # Not sure why there's some whitespaces but OK\n",
    "        word  = tokenizer.tokenize(sentence[i][0])\n",
    "        if len(word) == 0 :\n",
    "            continue\n",
    "        label = [sentence[i][2]]\n",
    "        if len(word)>1 :\n",
    "            label.extend([label[0].replace(\"B-\",\"I-\")]*(len(word)-1))\n",
    "#             print(word,label)\n",
    "#             input()\n",
    "        try : \n",
    "            assert(len(word)==len(label))\n",
    "        except : \n",
    "            print(\"+\"+sentence[i][0]+\"+\")\n",
    "            print(word,label)\n",
    "            raise Exception()\n",
    "        words.extend(word)\n",
    "        lls.extend(label)\n",
    "    tokenized_texts.append(words)\n",
    "    labels.append(lls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)\n",
    "\n",
    "# val_inputs = input_ids\n",
    "# val_tags   = tags\n",
    "# val_masks  = attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the model\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))\n",
    "model.load_state_dict(torch.load(\"ner.dataset.4.pth\",map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "if str(device) != 'cpu' : \n",
    "    model.cuda()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d02b2f42e6b4633b13e177e534aec78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "isent=0\n",
    "for i in tqdm(range(500)) : # tqdm(np.random.randint(0,len(val_inputs),500)) : # range(len(val_inputs))) :\n",
    "    input_ids       = torch.tensor([val_inputs[i].cpu().numpy()]).to(device)\n",
    "    tags            = torch.tensor([val_tags[i].cpu().numpy()]).to(device)\n",
    "    attention_masks = torch.tensor([val_masks[i].cpu().numpy()]).to(device)\n",
    "    \n",
    "    outputs   = model(input_ids, token_type_ids=None,\n",
    "                      attention_mask=attention_masks) # , labels=tags)\n",
    "    \n",
    "    ovar = outputs.to('cpu').detach().numpy()\n",
    "    \n",
    "    pred_labels = [np.argmax(ovar[0][j]) for j in range(len(ovar[0]))]\n",
    "    \n",
    "#     if i==6 :\n",
    "#         words   = tokenizer.convert_ids_to_tokens(val_inputs[6].cpu().numpy())\n",
    "#         my_true = tags.numpy()[0].tolist()\n",
    "#         my_pred = pred_labels\n",
    "#         for zz in zip(words,my_true,my_pred) :\n",
    "#             print(zz)\n",
    "    \n",
    "    y_true.extend(tags.numpy()[0].tolist())\n",
    "    y_pred.extend(pred_labels)\n",
    "    \n",
    "    isent+=1 \n",
    "    \n",
    "#     print(tags.numpy()[0])\n",
    "#     print(pred_labels)\n",
    "#     input()\n",
    "        \n",
    "#     input()\n",
    "\n",
    "  # loss, scores = outputs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.33      0.25      0.29         4\n",
      "       B-eve       0.33      0.25      0.29         4\n",
      "       B-geo       0.85      0.93      0.89       393\n",
      "       B-gpe       0.96      0.93      0.95       158\n",
      "       B-nat       1.00      0.33      0.50         3\n",
      "       B-org       0.79      0.67      0.72       229\n",
      "       B-per       0.83      0.89      0.86       175\n",
      "       B-tim       0.94      0.92      0.93       238\n",
      "       I-art       0.22      0.33      0.27         6\n",
      "       I-eve       0.00      0.00      0.00         5\n",
      "       I-geo       0.78      0.88      0.83       254\n",
      "       I-gpe       1.00      0.56      0.71         9\n",
      "       I-nat       0.33      1.00      0.50         1\n",
      "       I-org       0.84      0.62      0.72       396\n",
      "       I-per       0.85      0.94      0.89       441\n",
      "       I-tim       0.80      0.64      0.71       113\n",
      "           O       1.00      1.00      1.00     35071\n",
      "\n",
      "    accuracy                           0.99     37500\n",
      "   macro avg       0.70      0.66      0.65     37500\n",
      "weighted avg       0.99      0.99      0.99     37500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert back to label representations\n",
    "y_true = [idx2tag[i] for i in y_true]\n",
    "y_pred = [idx2tag[i] for i in y_pred]\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also get the results without \"O\"\n",
    "y_true_filt = []\n",
    "y_pred_filt = []\n",
    "for i in range(len(y_true)) :\n",
    "    if y_true[i] == \"O\" : \n",
    "        continue\n",
    "    y_true_filt.append(y_true[i])\n",
    "    y_pred_filt.append(y_pred[i])\n",
    "    \n",
    "print(classification_report(y_true_filt, y_pred_filt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to write a little routine that takes in a sentence and returns an array of the tokens along with the appropriate labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "model     = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))\n",
    "model.load_state_dict(torch.load(\"ner.dataset.4.pth\",map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "print(\"Model ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 75\n",
    "bs      = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_str_token(s) : \n",
    "    istart=0\n",
    "    iend=0\n",
    "    ovec=[]\n",
    "    for i in range(len(s)) : \n",
    "        if str(s[i]).lower() in list('abcdefghijklmnopqrstuvwxyz1234567890') :\n",
    "            continue\n",
    "        elif s[i] in list('-/.?,!%*') :\n",
    "            \n",
    "            # Capture the term before it\n",
    "            iend   = i\n",
    "            ovec.append([s[istart:iend],istart,iend])\n",
    "            istart = iend+1\n",
    "            \n",
    "            # Now capture the punctuation\n",
    "            istart = i\n",
    "            iend   = i+1\n",
    "            ovec.append([s[i],istart,iend])\n",
    "            istart = i+1\n",
    "            continue\n",
    "        else : \n",
    "            iend   = i\n",
    "            ovec.append([s[istart:iend],istart,iend])\n",
    "            istart = iend+1\n",
    "    return ovec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_converter(tt) :\n",
    "    ts = []\n",
    "    for itt in tt :\n",
    "        tok = 0\n",
    "        try : \n",
    "            tok = tokenizer.convert_tokens_to_ids([itt])[0]\n",
    "        except :\n",
    "            pass\n",
    "        ts.append(tok)\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"sentence\": \"The U.S. Environmental Protection Agency celebrated Earth Day this week in Washington ( 22 April ) by showcasing environmentally friendly new designs that could be the wave of the future .\",\n",
      " \"predictions\": [\n",
      "  {\n",
      "   \"token\": \"the\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"u\",\n",
      "   \"label\": \"B-org\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \".\",\n",
      "   \"label\": \"I-org\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"s\",\n",
      "   \"label\": \"I-org\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \".\",\n",
      "   \"label\": \"I-org\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"environmental\",\n",
      "   \"label\": \"I-org\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"protection\",\n",
      "   \"label\": \"I-org\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"agency\",\n",
      "   \"label\": \"I-org\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"celebrated\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"earth\",\n",
      "   \"label\": \"B-eve\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"day\",\n",
      "   \"label\": \"I-eve\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"this\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"week\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"in\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"washington\",\n",
      "   \"label\": \"B-geo\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"(\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"22\",\n",
      "   \"label\": \"B-tim\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"april\",\n",
      "   \"label\": \"I-tim\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \")\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"by\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"showcasing\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"environmentally\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"friendly\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"new\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"designs\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"that\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"could\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"be\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"the\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"wave\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"of\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"the\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \"future\",\n",
      "   \"label\": \"O\"\n",
      "  },\n",
      "  {\n",
      "   \"token\": \".\",\n",
      "   \"label\": \"O\"\n",
      "  }\n",
      " ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Break up the sentence into tokens\n",
    "sentence       = sentences[43000]\n",
    "\n",
    "# Ideally it would be good to have a pre-tokenzied sentence provided\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(sentence)\n",
    "input_id       = pad_sequences([tokenizer.convert_tokens_to_ids(tokenized_text)],maxlen=MAX_LEN, \n",
    "                               dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "attention_mask = [[float(i>0) for i in ii] for ii in input_id]\n",
    "input_id       = torch.tensor(input_id)\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "outputs        = model(input_id,token_type_ids=None,attention_mask=attention_mask)\n",
    "ovar           = outputs.to('cpu').detach().numpy()\n",
    "\n",
    "# Have to filter out the padding\n",
    "pred_labels = np.array([np.argmax(ovar[0][j]) for j in range(len(ovar[0]))])\n",
    "\n",
    "# pred_labels = pred_labels[(input_id.numpy()[0]>0)]\n",
    "pred_labels = pred_labels[0:len(tokenized_text)]\n",
    "\n",
    "# Sanity check\n",
    "assert(len(pred_labels)==len(tokenized_text))\n",
    "\n",
    "# Convert the labels back to text representation\n",
    "txt_labels = [idx2tag[i] for i in pred_labels]\n",
    "\n",
    "# Create an output JSON for the tokenized text\n",
    "odict = {'sentence':sentence,'predictions':[]}\n",
    "for tt in zip(tokenized_text,txt_labels) :\n",
    "    odict['predictions'].append({\n",
    "        'token' : tt[0],\n",
    "        'label' : tt[1]\n",
    "    })\n",
    "print(json.dumps(odict,indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
